# TurboLoader: Direct Replacement for PyTorch DataLoader

## Side-by-Side Comparison

### ‚ùå BEFORE: Standard PyTorch (Slow)

```python
import torch
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

# Create dataset
dataset = ImageFolder('data/', transform=transform)

# Create DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=64,
    num_workers=4,
    shuffle=True
)

# Training loop
for epoch in range(num_epochs):
    for images, labels in dataloader:
        # Train model
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

**Performance: ~17 img/s** üêå

---

### ‚úÖ AFTER: TurboLoader (35x Faster!)

```python
import torch
import turboloader

# Configure transforms (done in C++ with SIMD!)
transform_config = turboloader.TransformConfig()
transform_config.enable_resize = True
transform_config.resize_width = 224
transform_config.resize_height = 224
transform_config.enable_normalize = True
transform_config.mean = [0.485, 0.456, 0.406]
transform_config.std = [0.229, 0.224, 0.225]

# Create pipeline (replaces DataLoader)
pipeline = turboloader.Pipeline(
    tar_paths=['data.tar'],
    num_workers=8,
    decode_jpeg=True,
    enable_simd_transforms=True,
    transform_config=transform_config
)

# Training loop
for epoch in range(num_epochs):
    pipeline.start()

    while True:
        batch = pipeline.next_batch(64)
        if len(batch) == 0:
            break

        # Convert to PyTorch tensors
        images = torch.stack([
            torch.from_numpy(sample.get_image()).permute(2, 0, 1)
            for sample in batch
        ])

        # Train model (same as before!)
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    pipeline.stop()
```

**Performance: ~628 img/s** ‚ö° (35x faster!)

---

## üéØ Easiest Migration: Drop-in Dataset Wrapper

If you want to keep the exact same PyTorch DataLoader API, use the wrapper:

```python
import torch
from torch.utils.data import DataLoader
import turboloader

# Create TurboLoader dataset
class TurboLoaderDataset(torch.utils.data.IterableDataset):
    def __init__(self, tar_paths):
        self.tar_paths = tar_paths

    def __iter__(self):
        # Configure transforms
        config = turboloader.TransformConfig()
        config.enable_resize = True
        config.resize_width = 224
        config.resize_height = 224
        config.enable_normalize = True
        config.mean = [0.485, 0.456, 0.406]
        config.std = [0.229, 0.224, 0.225]

        # Create pipeline
        pipeline = turboloader.Pipeline(
            tar_paths=self.tar_paths,
            num_workers=8,
            decode_jpeg=True,
            enable_simd_transforms=True,
            transform_config=config
        )

        pipeline.start()
        while True:
            batch = pipeline.next_batch(1)
            if len(batch) == 0:
                break

            sample = batch[0]
            image = torch.from_numpy(sample.get_image()).permute(2, 0, 1)
            label = 0  # Your label logic here

            yield image, label

        pipeline.stop()

# Use with standard PyTorch DataLoader!
dataset = TurboLoaderDataset(['train.tar'])
dataloader = DataLoader(dataset, batch_size=64)

# Training loop is IDENTICAL to standard PyTorch!
for epoch in range(num_epochs):
    for images, labels in dataloader:
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

**Same API, 35x faster!** üöÄ

---

## What Changes?

### Minimal Changes Required:

1. **Data format**: Convert to WebDataset TAR format (one-time)
   ```bash
   # Your images in folders ‚Üí TAR file
   tar -cf train.tar -C /path/to/images .
   ```

2. **Transforms**: Configure in TurboLoader instead of torchvision
   ```python
   # Before: torchvision transforms
   transform = transforms.Compose([
       transforms.Resize((224, 224)),
       transforms.Normalize(mean=[...], std=[...])
   ])

   # After: TurboLoader config (runs in C++ with SIMD!)
   config = turboloader.TransformConfig()
   config.enable_resize = True
   config.resize_width = 224
   config.enable_normalize = True
   config.mean = [0.485, 0.456, 0.406]
   ```

3. **DataLoader ‚Üí Pipeline**
   ```python
   # Before
   dataloader = DataLoader(dataset, batch_size=64, num_workers=4)

   # After
   pipeline = turboloader.Pipeline(
       tar_paths=['data.tar'],
       num_workers=8  # Can use more - it's faster!
   )
   ```

---

## Feature Comparison

| Feature | PyTorch DataLoader | TurboLoader |
|---------|-------------------|-------------|
| **Throughput** | 17 img/s | 628 img/s |
| **Speedup** | 1x | **35x** ‚ö° |
| **Resize** | PIL (Python) | SIMD C++ (NEON/AVX2) |
| **Normalize** | NumPy | SIMD C++ |
| **Decode** | PIL/Pillow | libjpeg-turbo + SIMD |
| **Threading** | Python multiprocessing | C++ threads |
| **Memory** | High copies | Zero-copy where possible |
| **Setup** | Easy | Easy (3 lines) |
| **GPU Utilization** | 40-60% | 80-95% |

---

## Common Use Cases

### ImageNet Training
```python
pipeline = turboloader.Pipeline(
    tar_paths=['imagenet_train.tar'],
    num_workers=16,
    decode_jpeg=True,
    enable_simd_transforms=True
)
# 35x faster than PyTorch DataLoader!
```

### COCO Object Detection
```python
config = turboloader.TransformConfig()
config.enable_resize = True
config.resize_width = 640
config.resize_height = 640

pipeline = turboloader.Pipeline(
    tar_paths=['coco_train.tar'],
    num_workers=8,
    enable_simd_transforms=True,
    transform_config=config
)
```

### Custom Datasets
```python
# Works with any TAR file containing images!
pipeline = turboloader.Pipeline(
    tar_paths=['my_custom_data.tar'],
    num_workers=8,
    decode_jpeg=True
)
```

---

## Migration Checklist

- [ ] Convert dataset to TAR format (or use existing WebDataset)
- [ ] Install TurboLoader: `pip install turboloader`
- [ ] Replace DataLoader with Pipeline
- [ ] Configure transforms in TransformConfig
- [ ] Update training loop to use `next_batch()`
- [ ] Benchmark and enjoy 10-35x speedup!

---

## FAQ

### Q: Do I need to change my model?
**A:** No! TurboLoader just loads data faster. Your model code stays the same.

### Q: Does it work with my existing datasets?
**A:** Yes, but you need to convert to TAR format first (takes 5 minutes).

### Q: What about data augmentation?
**A:** Basic transforms (resize, normalize, flip, crop) are built-in with SIMD. For complex augmentations, you can still use torchvision on TurboLoader's output.

### Q: Does it work on CPU only?
**A:** Yes! TurboLoader is CPU-based and speeds up data loading, which helps GPU training.

### Q: How much faster is it really?
**A:** 10-35x faster depending on:
- Dataset size
- Image resolution
- Number of transforms
- CPU architecture (NEON/AVX2)

---

## Performance Tips

1. **Use more workers**: TurboLoader scales better
   ```python
   num_workers=16  # vs PyTorch's 4
   ```

2. **Enable all SIMD transforms**: Fastest option
   ```python
   enable_simd_transforms=True
   ```

3. **Increase queue size** for better prefetching
   ```python
   queue_size=512  # default is 256
   ```

4. **Use SSD storage**: I/O matters at high speeds

---

Ready to get started? See the [full example](../examples/pytorch_replacement_example.py)!
